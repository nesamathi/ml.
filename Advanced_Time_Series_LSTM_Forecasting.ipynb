{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nesamathi/ml./blob/main/Advanced_Time_Series_LSTM_Forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdwUpxk4_SUY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from hyperopt.pyll.base import scope\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm # For nice progress bars\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDJtZhs5_hyY",
        "outputId": "07ba8d39-c042-4db1-a7f8-8559ed15abb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Data Shape: (5000, 2)\n",
            "X_train shape: torch.Size([2982, 30, 2])\n"
          ]
        }
      ],
      "source": [
        "def generate_synthetic_data(n_samples=5000):\n",
        "    \"\"\"Generates a multi-variate time series with clear seasonality and trend.\"\"\"\n",
        "\n",
        "    # 1. Time Index\n",
        "    t = np.arange(n_samples)\n",
        "\n",
        "    # 2. Base Trend (e.g., exponential growth)\n",
        "    trend = 0.0001 * t**2 + 50\n",
        "\n",
        "    # 3. Long-term Seasonality (e.g., yearly)\n",
        "    seasonality_long = 10 * np.sin(t * 2 * np.pi / 365)\n",
        "\n",
        "    # 4. Short-term Seasonality (e.g., weekly)\n",
        "    seasonality_short = 3 * np.sin(t * 2 * np.pi / 30)\n",
        "\n",
        "    # 5. Noise Component\n",
        "    noise = np.random.normal(0, 1, n_samples)\n",
        "\n",
        "    # 6. Primary Target Series (Y1)\n",
        "    Y1 = trend + seasonality_long + seasonality_short + noise\n",
        "\n",
        "    # 7. Related Feature Series (Y2 - dependent on Y1 with some lag/shift)\n",
        "    Y2 = 0.5 * Y1 + 5 * np.cos(t * 2 * np.pi / 180) + np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "    # Combine into DataFrame\n",
        "    df = pd.DataFrame({'Target_Y1': Y1, 'Feature_Y2': Y2}, index=t)\n",
        "    return df\n",
        "\n",
        "# Get the data\n",
        "data_df = generate_synthetic_data()\n",
        "print(\"Generated Data Shape:\", data_df.shape)\n",
        "#\n",
        "\n",
        "#### B. Scaling and Sequence Windowing\n",
        "\n",
        "def preprocess_data(df, lookback=30):\n",
        "    \"\"\"Scales data and creates LSTM input sequences.\"\"\"\n",
        "\n",
        "    # 1. Scaling\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    data_scaled = scaler.fit_transform(df.values)\n",
        "\n",
        "    # 2. Sequence Windowing (Creates X, Y sequences for time series)\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data_scaled) - lookback):\n",
        "        # X: lookback time steps (e.g., past 30 days) for all features\n",
        "        X.append(data_scaled[i:(i + lookback)])\n",
        "        # Y: the next single time step (day 31) for only the target (Y1)\n",
        "        # Note: We assume Y1 (index 0) is the target for single-step forecasting\n",
        "        Y.append(data_scaled[i + lookback, 0])\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    # 3. Train/Validation/Test Split\n",
        "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, shuffle=False)\n",
        "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "    # 4. Convert to PyTorch Tensors\n",
        "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_train_t = torch.tensor(Y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
        "    Y_val_t = torch.tensor(Y_val, dtype=torch.float32).unsqueeze(1)\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "    Y_test_t = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    return X_train_t, Y_train_t, X_val_t, Y_val_t, X_test_t, Y_test_t, scaler, lookback\n",
        "\n",
        "# Execute preprocessing\n",
        "LOOKBACK = 30\n",
        "X_train, Y_train, X_val, Y_val, X_test, Y_test, scaler, LOOKBACK = preprocess_data(data_df, LOOKBACK)\n",
        "print(f\"X_train shape: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM4UoGfzACez"
      },
      "outputs": [],
      "source": [
        "class LSTMForecaster(nn.Module):\n",
        "    \"\"\"\n",
        "    A generic LSTM model for time series forecasting.\n",
        "    Parameters are dynamic to allow for hyperparameter tuning.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "        super(LSTMForecaster, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # LSTM Layer (Handles layer_depth, unit_count, and dropout)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Fully Connected Layer (Output is a single forecast value)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, input_size)\n",
        "\n",
        "        # Initialize hidden state and cell state (optional, but good practice)\n",
        "        # h0/c0 shape: (num_layers, batch_size, hidden_size)\n",
        "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # LSTM output: (output, (hn, cn))\n",
        "        # output shape: (batch_size, seq_len, hidden_size)\n",
        "        out, _ = self.lstm(x) # (x, (h0, c0)) if initializing\n",
        "\n",
        "        # Take the output of the last time step\n",
        "        # out[:, -1, :] shape: (batch_size, hidden_size)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        # out shape: (batch_size, 1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXwH_7_CAaR9"
      },
      "outputs": [],
      "source": [
        "def inverse_scale(Y_scaled, scaler, target_feature_index=0):\n",
        "    \"\"\"Inversely scales the target prediction (Y1) back to original units.\"\"\"\n",
        "    # Create a dummy array matching the input features but filling with the scaled target\n",
        "    Y_dummy = np.zeros((Y_scaled.shape[0], scaler.n_features_in_))\n",
        "    Y_dummy[:, target_feature_index] = Y_scaled.flatten()\n",
        "\n",
        "    # Inverse transform\n",
        "    Y_original = scaler.inverse_transform(Y_dummy)[:, target_feature_index]\n",
        "    return Y_original\n",
        "\n",
        "def evaluate_model(model, X_data, Y_data, scaler, lookback, batch_size=64):\n",
        "    \"\"\"Calculates RMSE and MAPE for a given dataset.\"\"\"\n",
        "    model.eval()\n",
        "    data_loader = DataLoader(TensorDataset(X_data, Y_data), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_batch in data_loader:\n",
        "            pred = model(X_batch)\n",
        "            predictions.append(pred.cpu().numpy())\n",
        "            actuals.append(Y_batch.cpu().numpy())\n",
        "\n",
        "    Y_pred_scaled = np.concatenate(predictions)\n",
        "    Y_true_scaled = np.concatenate(actuals)\n",
        "\n",
        "    # 1. Inverse Scale to original units for RMSE and MAPE\n",
        "    Y_pred = inverse_scale(Y_pred_scaled, scaler)\n",
        "    Y_true = inverse_scale(Y_true_scaled, scaler)\n",
        "\n",
        "    # 2. Calculate RMSE (Root Mean Squared Error)\n",
        "    rmse = np.sqrt(np.mean((Y_true - Y_pred)**2))\n",
        "\n",
        "    # 3. Calculate MAPE (Mean Absolute Percentage Error)\n",
        "    # Avoid division by zero by checking Y_true > 0\n",
        "    mape = np.mean(np.abs((Y_true - Y_pred) / Y_true)) * 100\n",
        "\n",
        "    return rmse, mape, Y_pred, Y_true\n",
        "\n",
        "def train_model(model, X_train, Y_train, X_val, Y_val, scaler, config):\n",
        "    \"\"\"Handles the training loop.\"\"\"\n",
        "\n",
        "    # Hyperparameters from config\n",
        "    n_epochs = config['n_epochs']\n",
        "    batch_size = config['batch_size']\n",
        "    learning_rate = config['learning_rate']\n",
        "\n",
        "    # Setup\n",
        "    train_data = TensorDataset(X_train, Y_train)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for X_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation Loss check\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_output = model(X_val)\n",
        "            val_loss = criterion(val_output, Y_val).item()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "    return best_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDsQogiEArv4",
        "outputId": "fa983d53-07b5-4914-a208-bcaec860a14e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## üöÄ Training Baseline Model\n",
            "Baseline Validation Loss: 0.0024\n",
            "Baseline Test RMSE: 634.40\n",
            "Baseline Test MAPE: 27.37%\n"
          ]
        }
      ],
      "source": [
        "## Baseline Configuration\n",
        "BASELINE_CONFIG = {\n",
        "    'input_size': X_train.shape[2],\n",
        "    'hidden_size': 64,\n",
        "    'num_layers': 2,\n",
        "    'dropout': 0.1,\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': 32,\n",
        "    'n_epochs': 50 # Reduced for a quick run\n",
        "}\n",
        "\n",
        "print(\"\\n## üöÄ Training Baseline Model\")\n",
        "\n",
        "# 1. Initialize Baseline Model\n",
        "baseline_model = LSTMForecaster(\n",
        "    input_size=BASELINE_CONFIG['input_size'],\n",
        "    hidden_size=BASELINE_CONFIG['hidden_size'],\n",
        "    num_layers=BASELINE_CONFIG['num_layers'],\n",
        "    dropout=BASELINE_CONFIG['dropout']\n",
        ")\n",
        "\n",
        "# 2. Train Baseline\n",
        "baseline_val_loss = train_model(baseline_model, X_train, Y_train, X_val, Y_val, scaler, BASELINE_CONFIG)\n",
        "\n",
        "# 3. Evaluate Baseline on Hold-out Test Set\n",
        "baseline_rmse, baseline_mape, _, _ = evaluate_model(\n",
        "    baseline_model, X_test, Y_test, scaler, LOOKBACK\n",
        ")\n",
        "\n",
        "print(f\"Baseline Validation Loss: {baseline_val_loss:.4f}\")\n",
        "print(f\"Baseline Test RMSE: {baseline_rmse:.2f}\")\n",
        "print(f\"Baseline Test MAPE: {baseline_mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvE5o3VkB1w5",
        "outputId": "c551f5ae-c1f4-40d1-b472-4a01006db090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## ‚öôÔ∏è Running Bayesian Optimization for 20 Trials\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [59:11<00:00, 177.58s/trial, best loss: 1.4928265045455191e-05]\n",
            "\n",
            "Optimal Hyperparameters Found:\n",
            "{'batch_size': 32, 'dropout': 0.21695369771814071, 'hidden_size': 224, 'learning_rate': 0.0009638919613666704, 'n_epochs': 50, 'num_layers': 1, 'input_size': 2}\n"
          ]
        }
      ],
      "source": [
        "## Hyperparameter Space Definition\n",
        "search_space = {\n",
        "    'hidden_size': scope.int(hp.quniform('hidden_size', 32, 256, 32)),\n",
        "    'num_layers': scope.int(hp.quniform('num_layers', 1, 4, 1)),\n",
        "    'dropout': hp.uniform('dropout', 0.0, 0.5),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(1e-4), np.log(1e-2)),\n",
        "    'batch_size': scope.int(hp.choice('batch_size', [16, 32, 64])),\n",
        "    'n_epochs': 30 # Fixed for tuning iterations\n",
        "}\n",
        "\n",
        "# Global constants for the objective function\n",
        "INPUT_SIZE = X_train.shape[2]\n",
        "N_TRIALS = 20 # Number of hyperopt iterations (can be increased for better results)\n",
        "\n",
        "def objective_function(params):\n",
        "    \"\"\"Objective function to minimize (Validation Loss).\"\"\"\n",
        "\n",
        "    # Initialize model with current parameters\n",
        "    model = LSTMForecaster(\n",
        "        input_size=INPUT_SIZE,\n",
        "        hidden_size=params['hidden_size'],\n",
        "        num_layers=params['num_layers'],\n",
        "        dropout=params['dropout']\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    val_loss = train_model(model, X_train, Y_train, X_val, Y_val, scaler, params)\n",
        "\n",
        "    # Return dictionary required by Hyperopt\n",
        "    return {\n",
        "        'loss': val_loss,\n",
        "        'status': STATUS_OK,\n",
        "        'params': params\n",
        "    }\n",
        "\n",
        "print(f\"\\n## ‚öôÔ∏è Running Bayesian Optimization for {N_TRIALS} Trials\")\n",
        "\n",
        "# 1. Run Hyperopt optimization\n",
        "trials = Trials()\n",
        "best_params = fmin(\n",
        "    fn=objective_function,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=N_TRIALS,\n",
        "    trials=trials,\n",
        "    show_progressbar=True\n",
        ")\n",
        "\n",
        "# 2. Get the optimal parameters\n",
        "best_config = trials.best_trial['result']['params']\n",
        "best_config['input_size'] = INPUT_SIZE\n",
        "best_config['n_epochs'] = 50 # Use more epochs for final training\n",
        "print(\"\\nOptimal Hyperparameters Found:\")\n",
        "print(best_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "KDIns_0LCmjK",
        "outputId": "df43a068-3459-4e2a-b2c7-489da2cec5e0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-36410370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define INPUT_SIZE and best_config based on the output of the hyperparameter optimization cell (xvE5o3VkB1w5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mINPUT_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m best_config = {\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define INPUT_SIZE and best_config based on the output of the hyperparameter optimization cell (xvE5o3VkB1w5)\n",
        "INPUT_SIZE = X_train.shape[2]\n",
        "best_config = {\n",
        "    'batch_size': 32,\n",
        "    'dropout': 0.21695369771814071,\n",
        "    'hidden_size': 224,\n",
        "    'learning_rate': 0.0009638919613666704,\n",
        "    'n_epochs': 50,\n",
        "    'num_layers': 1,\n",
        "    'input_size': INPUT_SIZE # Ensure input_size is also correctly set\n",
        "}\n",
        "\n",
        "class LSTMForecaster(nn.Module):\n",
        "    \"\"\"\n",
        "    A generic LSTM model for time series forecasting.\n",
        "    Parameters are dynamic to allow for hyperparameter tuning.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "        super(LSTMForecaster, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # LSTM Layer (Handles layer_depth, unit_count, and dropout)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Fully Connected Layer (Output is a single forecast value)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, input_size)\n",
        "\n",
        "        # Initialize hidden state and cell state (optional, but good practice)\n",
        "        # h0/c0 shape: (num_layers, batch_size, hidden_size)\n",
        "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # LSTM output: (output, (hn, cn))\n",
        "        # output shape: (batch_size, seq_len, hidden_size)\n",
        "        out, _ = self.lstm(x) # (x, (h0, c0)) if initializing\n",
        "\n",
        "        # Take the output of the last time step\n",
        "        # out[:, -1, :] shape: (batch_size, hidden_size)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        # out shape: (batch_size, 1)\n",
        "        return out\n",
        "\n",
        "print(\"\\n## üèÜ Training Final Optimized Model\")\n",
        "\n",
        "# 1. Initialize and Train Optimized Model\n",
        "optimized_model = LSTMForecaster(\n",
        "    input_size=best_config['input_size'],\n",
        "    hidden_size=best_config['hidden_size'],\n",
        "    num_layers=best_config['num_layers'],\n",
        "    dropout=best_config['dropout']\n",
        ")\n",
        "\n",
        "# The train_model function will use the learning rate/batch size from best_config\n",
        "final_val_loss = train_model(optimized_model, X_train, Y_train, X_val, Y_val, scaler, best_config)\n",
        "\n",
        "# 2. Evaluate Optimized Model on Test Set\n",
        "optimized_rmse, optimized_mape, opt_pred, opt_true = evaluate_model(\n",
        "    optimized_model, X_test, Y_test, scaler, LOOKBACK\n",
        ")\n",
        "\n",
        "print(f\"Optimized Validation Loss: {final_val_loss:.4f}\")\n",
        "print(f\"Optimized Test RMSE: {optimized_rmse:.2f}\")\n",
        "print(f\"Optimized Test MAPE: {optimized_mape:.2f}%\")\n",
        "\n",
        "# 3. Comprehensive Comparative Analysis (For Report)\n",
        "comparison_table = pd.DataFrame({\n",
        "    'Metric': ['RMSE', 'MAPE (%)'],\n",
        "    'Baseline': [baseline_rmse, baseline_mape],\n",
        "    'Optimized': [optimized_rmse, optimized_mape]\n",
        "})\n",
        "print(\"\\nComparative Performance Metrics:\")\n",
        "print(comparison_table.to_markdown(index=False))\n",
        "\n",
        "# Plotting the comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(opt_true, label='Actual Test Data')\n",
        "plt.plot(opt_pred, label='Optimized Forecast')\n",
        "plt.title('Optimized Model Forecast vs. Actuals (Test Set)')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Target Value (Original Units)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#\n",
        "\n",
        "# ---\n",
        "\n",
        "### 7. Time-Series Interpretability with SHAP (Task 4)\n",
        "\n",
        "# We adapt **SHAP (SHapley Additive exPlanations)** for the time series context to understand which input features and time steps were most influential in the model's prediction.\n",
        "\n",
        "# python\n",
        "# 1. Prepare SHAP data: A background set and the explanation set (test set)\n",
        "# SHAP requires a background dataset to estimate expected values.\n",
        "# Use a subset of the training data as the background.\n",
        "explainer_background = X_train[np.random.choice(X_train.shape[0], 100, replace=False)].cpu().numpy()\n",
        "\n",
        "# Use the test set for explanations\n",
        "explainer_test_data = X_test.cpu().numpy()\n",
        "\n",
        "# 2. Define the PyTorch prediction function for SHAP\n",
        "def pytorch_predictor(data):\n",
        "    \"\"\"Wrapper function to get predictions from the PyTorch model (numpy input, numpy output).\"\"\"\n",
        "    data_t = torch.tensor(data, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        preds_scaled = optimized_model(data_t).cpu().numpy()\n",
        "        # SHAP works best if the output is the raw prediction, not inverse scaled\n",
        "        return preds_scaled\n",
        "\n",
        "# 3. Create the SHAP Explainer\n",
        "# Use the DeepExplainer (optimized for Deep Learning models like LSTM)\n",
        "try:\n",
        "    explainer = shap.DeepExplainer(optimized_model, torch.tensor(explainer_background, dtype=torch.float32))\n",
        "\n",
        "    # 4. Calculate SHAP values for a subset of the test set (e.g., first 50 predictions)\n",
        "    shap_values = explainer.shap_values(torch.tensor(explainer_test_data[:50], dtype=torch.float32))[0]\n",
        "\n",
        "    # 5. Analyze and Visualize SHAP Values\n",
        "\n",
        "    # Convert feature indices to meaningful labels for the lookback window\n",
        "    # Example: F1_t-30, F2_t-30, ..., F1_t-1, F2_t-1\n",
        "    feature_names = data_df.columns.tolist() # ['Target_Y1', 'Feature_Y2']\n",
        "\n",
        "    # Create descriptive feature names for all (lookback * n_features) inputs\n",
        "    shap_feature_labels = []\n",
        "    for t_step in range(LOOKBACK, 0, -1): # t-30 to t-1\n",
        "        for feature_name in feature_names:\n",
        "            shap_feature_labels.append(f'{feature_name} (t-{t_step})')\n",
        "\n",
        "    # Reshape the SHAP values from (samples, seq_len, n_features) to (samples, seq_len * n_features)\n",
        "    # This matches the label list for the summary plot\n",
        "    shap_values_reshaped = shap_values.reshape(shap_values.shape[0], -1)\n",
        "\n",
        "    print(\"\\n## üìä SHAP Explainability Analysis\")\n",
        "\n",
        "    # Summary Plot: Shows overall feature importance and influence direction\n",
        "    shap.summary_plot(\n",
        "        shap_values_reshaped,\n",
        "        features=explainer_test_data[:50].reshape(50, -1),\n",
        "        feature_names=shap_feature_labels,\n",
        "        max_display=20 # Show top 20 most influential time/feature combinations\n",
        "    )\n",
        "    #\n",
        "    # Force Plot for a specific prediction (e.g., the first test sample)\n",
        "    shap.force_plot(\n",
        "        explainer.expected_value[0],\n",
        "        shap_values_reshaped[0, :],\n",
        "        explainer_test_data[0].reshape(-1),\n",
        "        feature_names=shap_feature_labels\n",
        "    )\n",
        "    #\n",
        "except Exception as e:\n",
        "    print(f\"\\nSHAP explanation failed. This can happen due to package conflicts or environment issues.\")\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d47ae868",
        "outputId": "2d5762f5-f672-48cf-e630-d28779ed8870"
      },
      "source": [
        "def generate_synthetic_data(n_samples=5000):\n",
        "    \"\"\"Generates a multi-variate time series with clear seasonality and trend.\"\"\"\n",
        "\n",
        "    # 1. Time Index\n",
        "    t = np.arange(n_samples)\n",
        "\n",
        "    # 2. Base Trend (e.g., exponential growth)\n",
        "    trend = 0.0001 * t**2 + 50\n",
        "\n",
        "    # 3. Long-term Seasonality (e.g., yearly)\n",
        "    seasonality_long = 10 * np.sin(t * 2 * np.pi / 365)\n",
        "\n",
        "    # 4. Short-term Seasonality (e.g., weekly)\n",
        "    seasonality_short = 3 * np.sin(t * 2 * np.pi / 30)\n",
        "\n",
        "    # 5. Noise Component\n",
        "    noise = np.random.normal(0, 1, n_samples)\n",
        "\n",
        "    # 6. Primary Target Series (Y1)\n",
        "    Y1 = trend + seasonality_long + seasonality_short + noise\n",
        "\n",
        "    # 7. Related Feature Series (Y2 - dependent on Y1 with some lag/shift)\n",
        "    Y2 = 0.5 * Y1 + 5 * np.cos(t * 2 * np.pi / 180) + np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "    # Combine into DataFrame\n",
        "    df = pd.DataFrame({'Target_Y1': Y1, 'Feature_Y2': Y2}, index=t)\n",
        "    return df\n",
        "\n",
        "# Get the data\n",
        "data_df = generate_synthetic_data()\n",
        "print(\"Generated Data Shape:\", data_df.shape)\n",
        "#\n",
        "\n",
        "#### B. Scaling and Sequence Windowing\n",
        "\n",
        "def preprocess_data(df, lookback=30):\n",
        "    \"\"\"Scales data and creates LSTM input sequences.\"\"\"\n",
        "\n",
        "    # 1. Scaling\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    data_scaled = scaler.fit_transform(df.values)\n",
        "\n",
        "    # 2. Sequence Windowing (Creates X, Y sequences for time series)\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data_scaled) - lookback):\n",
        "        # X: lookback time steps (e.g., past 30 days) for all features\n",
        "        X.append(data_scaled[i:(i + lookback)])\n",
        "        # Y: the next single time step (day 31) for only the target (Y1)\n",
        "        # Note: We assume Y1 (index 0) is the target for single-step forecasting\n",
        "        Y.append(data_scaled[i + lookback, 0])\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    # 3. Train/Validation/Test Split\n",
        "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, shuffle=False)\n",
        "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "    # 4. Convert to PyTorch Tensors\n",
        "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_train_t = torch.tensor(Y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
        "    Y_val_t = torch.tensor(Y_val, dtype=torch.float32).unsqueeze(1)\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "    Y_test_t = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    return X_train_t, Y_train_t, X_val_t, Y_val_t, X_test_t, Y_test_t, scaler, lookback\n",
        "\n",
        "# Execute preprocessing\n",
        "LOOKBACK = 30\n",
        "X_train, Y_train, X_val, Y_val, X_test, Y_test, scaler, LOOKBACK = preprocess_data(data_df, LOOKBACK)\n",
        "print(f\"X_train shape: {X_train.shape}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Data Shape: (5000, 2)\n",
            "X_train shape: torch.Size([2982, 30, 2])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmsHvJ+IjWy1r0tirFVgIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}